{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "list1=[] \n",
    "list2=[]\n",
    "\n",
    "# tweet objects were gotten from scraping \n",
    "for filename in glob.glob('tweet-objects\\*'):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        list1.append(data['text']) \n",
    "\n",
    "with open('./project-data/train.label.txt') as f:\n",
    "    reviews = f.readlines()\n",
    "    for x in reviews:\n",
    "        if 'nonrumour' in x:\n",
    "            list2.append(0)\n",
    "        else:\n",
    "            list2.append(1) #rumor or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=list1\n",
    "labels=list2\n",
    "\n",
    "training_size = int(len(sentences) * 0.8)\n",
    "\n",
    "training_sentences = sentences[0: training_size]\n",
    "testing_sentences = sentences[: training_size]\n",
    "\n",
    "training_labels = labels[0: training_size]\n",
    "testing_labels = labels[: training_size]\n",
    "\n",
    "training_labels_final = np.array(training_labels).astype('float32').reshape((-1,1))\n",
    "testing_labels_final = np.array(testing_labels).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "vocab_size = 8159\n",
    "embedding_dim = 128\n",
    "max_length = 280\n",
    "trunc_type='post' #or pre \n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenisation and padding\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_tweets=[]\n",
    "filtered_sentence=[]\n",
    "tokenizer = Tokenizer(num_words=None, filters='I\"#$%&()*+,-./:;<=>?@[\\J^_(I]N\\t\\n', lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "for words in training_sentences:\n",
    "    tokenized_tweets.append(words.lower())\n",
    "for w in tokenized_tweets:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(filtered_sentence) #vectorizing words\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(embedding_dim,\n",
    "                         return_sequences=True)\n",
    "))\n",
    "#model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "48/48 [==============================] - 23s 418ms/step - loss: 0.5444 - accuracy: 0.7769 - val_loss: 0.5271 - val_accuracy: 0.7796\n",
      "Epoch 2/7\n",
      "48/48 [==============================] - 20s 412ms/step - loss: 0.5319 - accuracy: 0.7799 - val_loss: 0.5255 - val_accuracy: 0.7822\n",
      "Epoch 3/7\n",
      "48/48 [==============================] - 20s 420ms/step - loss: 0.4171 - accuracy: 0.8255 - val_loss: 0.1524 - val_accuracy: 0.9711\n",
      "Epoch 4/7\n",
      "48/48 [==============================] - 21s 441ms/step - loss: 0.5280 - accuracy: 0.8227 - val_loss: 0.4785 - val_accuracy: 0.8240\n",
      "Epoch 5/7\n",
      "48/48 [==============================] - 21s 434ms/step - loss: 0.4085 - accuracy: 0.8556 - val_loss: 0.1045 - val_accuracy: 0.9817\n",
      "Epoch 6/7\n",
      "48/48 [==============================] - 20s 426ms/step - loss: 0.1057 - accuracy: 0.9809 - val_loss: 0.0807 - val_accuracy: 0.9829\n",
      "Epoch 7/7\n",
      "48/48 [==============================] - 21s 447ms/step - loss: 0.0701 - accuracy: 0.9869 - val_loss: 0.0509 - val_accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")\n",
    "num_epochs=7\n",
    "modelo = model.fit(training_padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final),\n",
    "          callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob('covid-objs\\*'):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        list1.append(data['text']) \n",
    "        \n",
    "padding_type='post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(list1)\n",
    "fakes_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=max_length)           \n",
    "\n",
    "classes = model.predict(fakes_padded)\n",
    "classes=classes.round()\n",
    "#print(classes)\n",
    "\n",
    "final=[]\n",
    "for nums in classes:\n",
    "    for n in nums:\n",
    "        final.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "rumour=['rumour']\n",
    "nonrumour=['nonrumour']\n",
    "with open('./new4.csv', 'w',newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for f in final:      \n",
    "        if f==1:\n",
    "            writer.writerow(rumour)\n",
    "        else:\n",
    "            writer.writerow(nonrumour)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
